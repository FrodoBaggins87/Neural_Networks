{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "o2cKzJZObJ7A",
        "DHB2tEIOcC_n",
        "LzsTMAuacNgW"
      ],
      "authorship_tag": "ABX9TyPASRHkDAT4DRW6WD4gLqHG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrodoBaggins87/Neural_Networks/blob/main/FoodDetector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setting up"
      ],
      "metadata": {
        "id": "o2cKzJZObJ7A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4HZ08RDahkX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba97a468-3d50-4611-a197-77d49c9b8e30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available libraries not updated, downloading updated libraries\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.1)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Torch version:2.3.1+cu121\n",
            "torchvision version:0.18.1+cu121\n"
          ]
        }
      ],
      "source": [
        "#we need Torch 1.12 + and Torchvision 0.13 + for this study\n",
        "try:\n",
        "  import torch, torchvision\n",
        "  assert int(torch.__version__.split(\".\")[1])>=12, \"Torch version should be 1.12 or above\"\n",
        "  assert int(torchvision.__version__.split(\".\")[1])>=13, \"Torch version should be 0.12 or above\"\n",
        "  print(f\"Torch version:{torch.__version__}\")\n",
        "  print(f\"torchvision version:{torchvision.__version__}\")\n",
        "except:\n",
        "  print(\"Available libraries not updated, downloading updated libraries\")\n",
        "  !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "  import torch, torchvision\n",
        "  print(f\"Torch version:{torch.__version__}\")\n",
        "  print(f\"torchvision version:{torchvision.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "try:\n",
        "  from torchinfo import summary\n",
        "except:\n",
        "  !pip install -q torchinfo\n",
        "  from torchinfo import summary"
      ],
      "metadata": {
        "id": "2HMejEWXbUcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Reusable scripts"
      ],
      "metadata": {
        "id": "HBGKV32obXbS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "data_setup.py"
      ],
      "metadata": {
        "id": "QOTdwgyDb3Y7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile data_setup.py\n",
        "import os\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "NUM_WORKERS= os.cpu_count()\n",
        "def create_dataloaders(\n",
        "    train_dir: str,\n",
        "    test_dir:str,\n",
        "    train_transform: transforms.Compose,\n",
        "    test_transform: transforms.Compose,\n",
        "    batch_size:int,\n",
        "    num_workers: int=NUM_WORKERS\n",
        "):\n",
        "\n",
        "  training_data=datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
        "  testing_data=datasets.ImageFolder(root=test_dir, transform=test_transform)\n",
        "  class_names=training_data.classes\n",
        "  train_dataloader=DataLoader(dataset=training_data,\n",
        "                              batch_size=batch_size,#sample per dataloader\n",
        "                              num_workers=num_workers,\n",
        "                              shuffle=True,\n",
        "                              pin_memory= True)\n",
        "  test_dataloader=DataLoader(dataset=testing_data,\n",
        "                            batch_size=batch_size,\n",
        "                            num_workers=num_workers,\n",
        "                            shuffle=False,\n",
        "                            pin_memory= True)\n",
        "  return train_dataloader, test_dataloader, class_names\n"
      ],
      "metadata": {
        "id": "4X8wkSXWbfVM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ecbcfd9-ae5d-44c5-819a-a8483a114d89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting data_setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "engine.py"
      ],
      "metadata": {
        "id": "-waZTEOBbzej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile engine.py\n",
        "import torch\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from typing import Dict, List, Tuple\n",
        "def train_step(model:torch.nn.Module,\n",
        "               dataloader:torch.utils.data.DataLoader,\n",
        "               loss_fn:torch.nn.Module,\n",
        "               optimizer:torch.optim.Optimizer,\n",
        "              device: torch.device) -> Tuple[float, float]:\n",
        "  #putting in training mode\n",
        "  model.train()\n",
        "  #setup training loss and training accuracy\n",
        "  train_loss,train_acc=0,0\n",
        "\n",
        "  for batch,(x,y) in enumerate(dataloader):\n",
        "    #send data to target device\n",
        "    x,y=x.to(device),y.to(device)\n",
        "    #forward pass\n",
        "    y_pred=model(x)\n",
        "    #calculate and accumulate losses\n",
        "    loss=loss_fn(y_pred,y)\n",
        "    train_loss+=loss.item()\n",
        "    #optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "    #loss backward\n",
        "    loss.backward()\n",
        "    #optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    #calculate and accumulate accuracy metric for all batches\n",
        "    y_pred_class=torch.argmax(torch.softmax(y_pred,dim=1),dim=1)\n",
        "    train_acc+=(y_pred_class==y).sum().item()/len(y_pred)\n",
        "\n",
        "  #getting average loss and accuracy for each batch\n",
        "  train_loss/=len(dataloader)\n",
        "  train_acc/=len(dataloader)\n",
        "  return train_loss, train_acc\n",
        "\n",
        "def test_step(model:torch.nn.Module,\n",
        "               dataloader:torch.utils.data.DataLoader,\n",
        "               loss_fn:torch.nn.Module,\n",
        "              device: torch.device) -> Tuple[float,float]:\n",
        "  #putting in eval mode\n",
        "  model.eval()\n",
        "  #setup test loss and test accuracy\n",
        "  test_loss,test_acc=0,0\n",
        "  #turn on inference context manager\n",
        "  with torch.inference_mode():\n",
        "    #loop through dataloader batches\n",
        "    for batch,(x,y) in enumerate(dataloader):\n",
        "      #send data to target device\n",
        "      x,y=x.to(device),y.to(device)\n",
        "      #forward pass\n",
        "      test_pred_logits=model(x)\n",
        "      #calculate and accumulate loss\n",
        "      loss=loss_fn(test_pred_logits,y)\n",
        "      test_loss+=loss.item()\n",
        "      #calculate and accumulate accuracy\n",
        "      test_pred_labels=torch.argmax(torch.softmax(test_pred_logits,dim=1),dim=1)\n",
        "      test_acc+=(test_pred_labels==y).sum().item()/len(test_pred_labels)#can probably also use len(test_pred), not sure both should work i think\n",
        "  #getting average loss and accuracy for each batch\n",
        "  test_acc/=len(dataloader)\n",
        "  test_loss/=len(dataloader)\n",
        "  return test_loss, test_acc\n",
        "\n",
        "#defining functions and various required parameters\n",
        "def train(model:torch.nn.Module,\n",
        "          train_dataloader:torch.utils.data.DataLoader,\n",
        "          test_dataloader:torch.utils.data.DataLoader,\n",
        "          optimizer:torch.optim.Optimizer,\n",
        "          loss_fn:torch.nn.Module,\n",
        "          epochs: int,\n",
        "        device: torch.device) -> Dict[str, list]:\n",
        "  #create empty results dictionary\n",
        "  results={\"train_loss\":[],\n",
        "           \"test_loss\":[],\n",
        "           \"train_acc\":[],\n",
        "           \"test_acc\":[]}\n",
        "  #looping through train_step() and test_step()\n",
        "  for epoch in tqdm(range(epochs)):\n",
        "    train_loss,train_acc=train_step(model=model,\n",
        "                                    dataloader=train_dataloader,\n",
        "                                    loss_fn=loss_fn,\n",
        "                                    optimizer=optimizer,\n",
        "                                    device=device)\n",
        "    test_loss, test_acc=test_step(model=model,\n",
        "                                  dataloader=test_dataloader,\n",
        "                                  loss_fn=loss_fn,\n",
        "                                  device=device)\n",
        "  #print whats happening\n",
        "    print(\n",
        "        f\"Epoch:{epoch+1}|\"\n",
        "        f\"Train Loss:{train_loss:.4f}|\"\n",
        "        f\"Training Accuracy: {train_acc:.4f}|\"\n",
        "        f\"Test Loss: {test_loss:.4f}|\"\n",
        "        f\"Test Accuracy: {test_acc:.4f}\"\n",
        "    )\n",
        "    #updating result dictionary\n",
        "    results[\"train_loss\"].append(train_loss)\n",
        "    results[\"test_loss\"].append(test_loss)\n",
        "    results[\"train_acc\"].append(train_acc)\n",
        "    results[\"test_acc\"].append(test_acc)\n",
        "  return results\n"
      ],
      "metadata": {
        "id": "IpicpT7bb0Gp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58b59f0f-31a5-45ea-ebc7-2065558961c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting engine.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "utils.py"
      ],
      "metadata": {
        "id": "MPmAPJRrb-A3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile utils.py\n",
        "import torch\n",
        "from pathlib import Path\n",
        "def save_model(model:torch.nn.Module,\n",
        "               target_dir: str,\n",
        "               model_name:str):\n",
        "  #creating target directory\n",
        "  target_dir_path=Path(target_dir)\n",
        "  target_dir_path.mkdir(parents=True,\n",
        "                        exist_ok=True)\n",
        "  #creating model save path\n",
        "  assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n",
        "  model_save_path = target_dir_path/model_name\n",
        "\n",
        "  #save the model state_dict\n",
        "  print(f\"Saving model to:{model_save_path}\")\n",
        "  torch.save(obj=model.state_dict(),\n",
        "             f=model_save_path)"
      ],
      "metadata": {
        "id": "IeXUN_Ekb9pJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a911da17-7936-4572-bf64-d956eeb73914"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "effnet_b2.py"
      ],
      "metadata": {
        "id": "sU8ebc3egW_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile effnet_b2.py\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "def create_effnet_b2(device:str,num_classes:int,seed:int=69):\n",
        "  #1\n",
        "  weights=torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
        "  transforms=weights.transforms()\n",
        "  model=torchvision.models.efficientnet_b2(weights=weights).to(device)\n",
        "\n",
        "  #2. freeze all parameters in all layers\n",
        "  for param in model.parameters():\n",
        "    param.requires_grad=False\n",
        "  #3. set random seeds\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "\n",
        "  #4. changing classifier layer\n",
        "  model.classifier= torch.nn.Sequential(nn.Dropout(p=0.2, inplace=True),\n",
        "                                        nn.Linear(in_features=1408,\n",
        "                                                  out_features=num_classes,\n",
        "                                                  bias=True).to(device))\n",
        "  #5. give name\n",
        "  model.name='effnet_b2'\n",
        "  print(f\"Making EfficientNet_B2\")\n",
        "\n",
        "  return model,weights,transforms"
      ],
      "metadata": {
        "id": "MLwLlKCIgX4G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec873a65-23fe-401c-c734-ad417e71b6b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting effnet_b2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Device agnostic code"
      ],
      "metadata": {
        "id": "DHB2tEIOcC_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "id": "lMYfmATBcKIV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5488d16e-6481-4ff4-ab1c-db00c955b37c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Random seed function"
      ],
      "metadata": {
        "id": "LzsTMAuacNgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#set seeds\n",
        "def set_seeds(seed:int=69):\n",
        "  \"set seed whenever required before torch operations. Default seed = 69\"\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)"
      ],
      "metadata": {
        "id": "7OljzLZYcRVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Getting an EffNet_B2 model"
      ],
      "metadata": {
        "id": "aWYcINGfiSZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import effnet_b2\n",
        "effnet_model,_,effnet_transforms=effnet_b2.create_effnet_b2(num_classes=101,device=device)\n"
      ],
      "metadata": {
        "id": "guKLY_MKiVFw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c68b12fe-9f1b-4bee-c772-4a83f65466cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Making EfficientNet_B2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "summary(effnet_model,\n",
        "        input_size=(1,3,224,224),\n",
        "        col_names=['input_size', 'output_size','num_params','trainable'])\n"
      ],
      "metadata": {
        "id": "cMnGk0kQkGdd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fa8ca28-3d05-47f7-ee63-35bad34fc836"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "===========================================================================================================================================================\n",
              "Layer (type:depth-idx)                                  Input Shape               Output Shape              Param #                   Trainable\n",
              "===========================================================================================================================================================\n",
              "EfficientNet                                            [1, 3, 224, 224]          [1, 101]                  --                        Partial\n",
              "├─Sequential: 1-1                                       [1, 3, 224, 224]          [1, 1408, 7, 7]           --                        False\n",
              "│    └─Conv2dNormActivation: 2-1                        [1, 3, 224, 224]          [1, 32, 112, 112]         --                        False\n",
              "│    │    └─Conv2d: 3-1                                 [1, 3, 224, 224]          [1, 32, 112, 112]         (864)                     False\n",
              "│    │    └─BatchNorm2d: 3-2                            [1, 32, 112, 112]         [1, 32, 112, 112]         (64)                      False\n",
              "│    │    └─SiLU: 3-3                                   [1, 32, 112, 112]         [1, 32, 112, 112]         --                        --\n",
              "│    └─Sequential: 2-2                                  [1, 32, 112, 112]         [1, 16, 112, 112]         --                        False\n",
              "│    │    └─MBConv: 3-4                                 [1, 32, 112, 112]         [1, 16, 112, 112]         (1,448)                   False\n",
              "│    │    └─MBConv: 3-5                                 [1, 16, 112, 112]         [1, 16, 112, 112]         (612)                     False\n",
              "│    └─Sequential: 2-3                                  [1, 16, 112, 112]         [1, 24, 56, 56]           --                        False\n",
              "│    │    └─MBConv: 3-6                                 [1, 16, 112, 112]         [1, 24, 56, 56]           (6,004)                   False\n",
              "│    │    └─MBConv: 3-7                                 [1, 24, 56, 56]           [1, 24, 56, 56]           (10,710)                  False\n",
              "│    │    └─MBConv: 3-8                                 [1, 24, 56, 56]           [1, 24, 56, 56]           (10,710)                  False\n",
              "│    └─Sequential: 2-4                                  [1, 24, 56, 56]           [1, 48, 28, 28]           --                        False\n",
              "│    │    └─MBConv: 3-9                                 [1, 24, 56, 56]           [1, 48, 28, 28]           (16,518)                  False\n",
              "│    │    └─MBConv: 3-10                                [1, 48, 28, 28]           [1, 48, 28, 28]           (43,308)                  False\n",
              "│    │    └─MBConv: 3-11                                [1, 48, 28, 28]           [1, 48, 28, 28]           (43,308)                  False\n",
              "│    └─Sequential: 2-5                                  [1, 48, 28, 28]           [1, 88, 14, 14]           --                        False\n",
              "│    │    └─MBConv: 3-12                                [1, 48, 28, 28]           [1, 88, 14, 14]           (50,300)                  False\n",
              "│    │    └─MBConv: 3-13                                [1, 88, 14, 14]           [1, 88, 14, 14]           (123,750)                 False\n",
              "│    │    └─MBConv: 3-14                                [1, 88, 14, 14]           [1, 88, 14, 14]           (123,750)                 False\n",
              "│    │    └─MBConv: 3-15                                [1, 88, 14, 14]           [1, 88, 14, 14]           (123,750)                 False\n",
              "│    └─Sequential: 2-6                                  [1, 88, 14, 14]           [1, 120, 14, 14]          --                        False\n",
              "│    │    └─MBConv: 3-16                                [1, 88, 14, 14]           [1, 120, 14, 14]          (149,158)                 False\n",
              "│    │    └─MBConv: 3-17                                [1, 120, 14, 14]          [1, 120, 14, 14]          (237,870)                 False\n",
              "│    │    └─MBConv: 3-18                                [1, 120, 14, 14]          [1, 120, 14, 14]          (237,870)                 False\n",
              "│    │    └─MBConv: 3-19                                [1, 120, 14, 14]          [1, 120, 14, 14]          (237,870)                 False\n",
              "│    └─Sequential: 2-7                                  [1, 120, 14, 14]          [1, 208, 7, 7]            --                        False\n",
              "│    │    └─MBConv: 3-20                                [1, 120, 14, 14]          [1, 208, 7, 7]            (301,406)                 False\n",
              "│    │    └─MBConv: 3-21                                [1, 208, 7, 7]            [1, 208, 7, 7]            (686,868)                 False\n",
              "│    │    └─MBConv: 3-22                                [1, 208, 7, 7]            [1, 208, 7, 7]            (686,868)                 False\n",
              "│    │    └─MBConv: 3-23                                [1, 208, 7, 7]            [1, 208, 7, 7]            (686,868)                 False\n",
              "│    │    └─MBConv: 3-24                                [1, 208, 7, 7]            [1, 208, 7, 7]            (686,868)                 False\n",
              "│    └─Sequential: 2-8                                  [1, 208, 7, 7]            [1, 352, 7, 7]            --                        False\n",
              "│    │    └─MBConv: 3-25                                [1, 208, 7, 7]            [1, 352, 7, 7]            (846,900)                 False\n",
              "│    │    └─MBConv: 3-26                                [1, 352, 7, 7]            [1, 352, 7, 7]            (1,888,920)               False\n",
              "│    └─Conv2dNormActivation: 2-9                        [1, 352, 7, 7]            [1, 1408, 7, 7]           --                        False\n",
              "│    │    └─Conv2d: 3-27                                [1, 352, 7, 7]            [1, 1408, 7, 7]           (495,616)                 False\n",
              "│    │    └─BatchNorm2d: 3-28                           [1, 1408, 7, 7]           [1, 1408, 7, 7]           (2,816)                   False\n",
              "│    │    └─SiLU: 3-29                                  [1, 1408, 7, 7]           [1, 1408, 7, 7]           --                        --\n",
              "├─AdaptiveAvgPool2d: 1-2                                [1, 1408, 7, 7]           [1, 1408, 1, 1]           --                        --\n",
              "├─Sequential: 1-3                                       [1, 1408]                 [1, 101]                  --                        True\n",
              "│    └─Dropout: 2-10                                    [1, 1408]                 [1, 1408]                 --                        --\n",
              "│    └─Linear: 2-11                                     [1, 1408]                 [1, 101]                  142,309                   True\n",
              "===========================================================================================================================================================\n",
              "Total params: 7,843,303\n",
              "Trainable params: 142,309\n",
              "Non-trainable params: 7,700,994\n",
              "Total mult-adds (M): 657.78\n",
              "===========================================================================================================================================================\n",
              "Input size (MB): 0.60\n",
              "Forward/backward pass size (MB): 156.80\n",
              "Params size (MB): 31.37\n",
              "Estimated Total Size (MB): 188.77\n",
              "==========================================================================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Make separate transform for training dataset"
      ],
      "metadata": {
        "id": "auzprawxk9Su"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "food_101_transform=transforms.Compose([transforms.TrivialAugmentWide(),\n",
        "                                       effnet_transforms])"
      ],
      "metadata": {
        "id": "hFUj0frXlBGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Training data transform:{food_101_transform}')\n",
        "print(f'Testing data transform:{effnet_transforms}')"
      ],
      "metadata": {
        "id": "P8O9LEIvlkjs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bff1fcd-eb8e-4841-ab1d-51003960a949"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data transform:Compose(\n",
            "    TrivialAugmentWide(num_magnitude_bins=31, interpolation=InterpolationMode.NEAREST, fill=None)\n",
            "    ImageClassification(\n",
            "    crop_size=[288]\n",
            "    resize_size=[288]\n",
            "    mean=[0.485, 0.456, 0.406]\n",
            "    std=[0.229, 0.224, 0.225]\n",
            "    interpolation=InterpolationMode.BICUBIC\n",
            ")\n",
            ")\n",
            "Testing data transform:ImageClassification(\n",
            "    crop_size=[288]\n",
            "    resize_size=[288]\n",
            "    mean=[0.485, 0.456, 0.406]\n",
            "    std=[0.229, 0.224, 0.225]\n",
            "    interpolation=InterpolationMode.BICUBIC\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Getting data"
      ],
      "metadata": {
        "id": "vLTJdUrKl7qr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#make data directory\n",
        "from pathlib import Path\n",
        "data_dir=Path('data')\n",
        "\n",
        "#Getting training data from Food101 dataset\n",
        "train_data=torchvision.datasets.Food101(root=data_dir,\n",
        "                                    split='train',\n",
        "                                    transform=food_101_transform,\n",
        "                                    download=True)\n",
        "\n",
        "#Get test data\n",
        "test_data=torchvision.datasets.Food101(root=data_dir,\n",
        "                                    split='test',\n",
        "                                    transform=effnet_transforms,\n",
        "                                    download=True)"
      ],
      "metadata": {
        "id": "UxtJpPELl9cB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "541ac4b2-131d-42f2-b77c-7149431b2f0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://data.vision.ee.ethz.ch/cvl/food-101.tar.gz to data/food-101.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4996278331/4996278331 [03:57<00:00, 21077515.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/food-101.tar.gz to data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data.classes)\n",
        "print(f'Number of training samples: {len(train_data)}')\n",
        "print(f'Number of testing samples:{len(test_data)}')"
      ],
      "metadata": {
        "id": "eABPeyzXm-Cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "229f82aa-f099-47a3-e126-b45afb382852"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['apple_pie', 'baby_back_ribs', 'baklava', 'beef_carpaccio', 'beef_tartare', 'beet_salad', 'beignets', 'bibimbap', 'bread_pudding', 'breakfast_burrito', 'bruschetta', 'caesar_salad', 'cannoli', 'caprese_salad', 'carrot_cake', 'ceviche', 'cheese_plate', 'cheesecake', 'chicken_curry', 'chicken_quesadilla', 'chicken_wings', 'chocolate_cake', 'chocolate_mousse', 'churros', 'clam_chowder', 'club_sandwich', 'crab_cakes', 'creme_brulee', 'croque_madame', 'cup_cakes', 'deviled_eggs', 'donuts', 'dumplings', 'edamame', 'eggs_benedict', 'escargots', 'falafel', 'filet_mignon', 'fish_and_chips', 'foie_gras', 'french_fries', 'french_onion_soup', 'french_toast', 'fried_calamari', 'fried_rice', 'frozen_yogurt', 'garlic_bread', 'gnocchi', 'greek_salad', 'grilled_cheese_sandwich', 'grilled_salmon', 'guacamole', 'gyoza', 'hamburger', 'hot_and_sour_soup', 'hot_dog', 'huevos_rancheros', 'hummus', 'ice_cream', 'lasagna', 'lobster_bisque', 'lobster_roll_sandwich', 'macaroni_and_cheese', 'macarons', 'miso_soup', 'mussels', 'nachos', 'omelette', 'onion_rings', 'oysters', 'pad_thai', 'paella', 'pancakes', 'panna_cotta', 'peking_duck', 'pho', 'pizza', 'pork_chop', 'poutine', 'prime_rib', 'pulled_pork_sandwich', 'ramen', 'ravioli', 'red_velvet_cake', 'risotto', 'samosa', 'sashimi', 'scallops', 'seaweed_salad', 'shrimp_and_grits', 'spaghetti_bolognese', 'spaghetti_carbonara', 'spring_rolls', 'steak', 'strawberry_shortcake', 'sushi', 'tacos', 'takoyaki', 'tiramisu', 'tuna_tartare', 'waffles']\n",
            "Number of training samples: 75750\n",
            "Number of testing samples:25250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make a subset of the dataset for faster expermenting"
      ],
      "metadata": {
        "id": "qCYB99mTn2wq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#make a function for splitting a dataset\n",
        "def split_dataset(dataset:torchvision.datasets, split_size:float=0.2, seed:int=69):\n",
        "  '''\n",
        "  Splits a given dataset to get a subset of the training dataset for faster expermentation\n",
        "  Put dataset in argument dataset and split_size in argument split_size\n",
        "  '''\n",
        "  length_1=int(len(dataset)*split_size)\n",
        "  length_2=len(dataset)-length_1\n",
        "\n",
        "  print(f\"Original dataset length:{len(dataset)}\")\n",
        "  print(f\"Length of first split:{length_1}\")\n",
        "  print(f\"Length of second split:{length_2}\")\n",
        "\n",
        "  #using torch.utils.data.random_split\n",
        "  random_split_1,random_split_2=torch.utils.data.random_split(dataset=dataset,\n",
        "                                                     lengths=[length_1,length_2],\n",
        "                                                     generator=torch.manual_seed(seed))\n",
        "  return random_split_1,random_split_2"
      ],
      "metadata": {
        "id": "6KrFvPwnSN4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#20% of training dataset\n",
        "train_data_20,_=split_dataset(dataset=train_data,\n",
        "                                         split_size=0.2,\n",
        "                                         seed=69)\n",
        "\n",
        "#20% of testing dataset\n",
        "test_data_20,_=split_dataset(dataset=test_data,\n",
        "                                         split_size=0.2,\n",
        "                                         seed=69)\n",
        "\n",
        "len(train_data_20),len(test_data_20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOVSQuaeVLnw",
        "outputId": "defbd11a-d26f-4b3b-b66a-e646a73106c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataset length:75750\n",
            "Length of first split:15150\n",
            "Length of second split:60600\n",
            "Original dataset length:25250\n",
            "Length of first split:5050\n",
            "Length of second split:20200\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15150, 5050)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_20"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8EDAjzqj3kN",
        "outputId": "d2176433-ff4e-46f7-96a1-e1641bdfb263"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataset.Subset at 0x7fc5ba6fd2d0>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Make dataloaders for train_data_20 and test_data_20"
      ],
      "metadata": {
        "id": "kQIlQwuHV3yC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "BATCH_SIZE=32\n",
        "NUM_WORKERS=os.cpu_count()\n",
        "\n",
        "#Creating training dataloader using torch.utils.data.DataLoader()\n",
        "train_dataloader_20=torch.utils.data.DataLoader(dataset=train_data_20,\n",
        "                                                batch_size=32,\n",
        "                                                shuffle=True,\n",
        "                                                num_workers=NUM_WORKERS)\n",
        "\n",
        "#Creating testing dataloader using torch.utils.data.DataLoader()\n",
        "test_dataloader_20=torch.utils.data.DataLoader(dataset=test_data_20,\n",
        "                                               batch_size=32,\n",
        "                                               shuffle=False,\n",
        "                                               num_workers=NUM_WORKERS)\n",
        "\n",
        "#making full class_names list\n",
        "class_names=train_data.classes"
      ],
      "metadata": {
        "id": "r_YB4r2XWD9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training classifier layer of eff_net_b2"
      ],
      "metadata": {
        "id": "syjTQqnshxZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Number of Trainable parameters=142,309\n",
        "import engine\n",
        "\n",
        "#set optimizer\n",
        "optim=torch.optim.Adam(params=effnet_model.parameters(),\n",
        "                    lr=0.001)\n",
        "#setup loss function\n",
        "loss_fn=torch.nn.CrossEntropyLoss()\n",
        "\n",
        "#train\n",
        "effnet_results=engine.train(model=effnet_model,\n",
        "                            train_dataloader=train_dataloader_20,\n",
        "                            test_dataloader=test_dataloader_20,\n",
        "                            optimizer=optimizer,\n",
        "                            loss_fn=loss_fn,\n",
        "                            epochs=5,\n",
        "                            device=device)\n"
      ],
      "metadata": {
        "id": "FXfqz_r7h2zR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Plot loss curves"
      ],
      "metadata": {
        "id": "gJk0BecGxwSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define function to take the values in th e dictionary and plot\n",
        "def plot_loss_curves(results):\n",
        "  train_loss=results['train_loss']\n",
        "  test_loss=results['test_loss']\n",
        "  train_accuracy=results['train_acc']\n",
        "  test_accuracy= results['test_acc']\n",
        "  epochs=range(len(results['train_loss']))\n",
        "  plt.figure(figsize=(16,8))\n",
        "\n",
        "  #plotting loss\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.plot(epochs, train_loss, label='train loss')\n",
        "  plt.plot(epochs,test_loss, label='test loss')\n",
        "  plt.title('Loss')\n",
        "  plt.xlabel('epochs')\n",
        "  plt.legend()\n",
        "  #plotting accuracy\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.plot(epochs, train_accuracy, label='train accuracy')\n",
        "  plt.plot(epochs,test_accuracy, label='test accuracy')\n",
        "  plt.title('Accuracy')\n",
        "  plt.xlabel('epochs')\n",
        "  plt.legend()"
      ],
      "metadata": {
        "id": "UT9tUvXPxzCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_curves(effnet_results)"
      ],
      "metadata": {
        "id": "m93Ero9Ux3hP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Saving the model"
      ],
      "metadata": {
        "id": "87diUAydyrlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#checking model size\n",
        "from pathlib import Path\n",
        "eff_net_b2_size=Path('models/pretrained_eff_net_b2_feature_extractor.pth').stat().st_size//(1024*1024)#division converts bytes to MBs\n",
        "print(f\"The Pretrained EfficientNet_B2 feature extractor has size: {eff_net_b2_size} MB\")"
      ],
      "metadata": {
        "id": "vaDTylE2ytYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#counting number of parameters\n",
        "num_parameters=sum(p.numel() for p in effnet_model.parameters())\n",
        "print(f\"The pretrained EfficientNet_B2 feature extractor has {num_parameters} parameters\")"
      ],
      "metadata": {
        "id": "C3GGR8lyzGVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#saving\n",
        "import utils\n",
        "utils.save_model(model=effnet_model,\n",
        "                 target_dir='models',\n",
        "                 model_name='pretrained_eff_net_feature_extractor.pth')"
      ],
      "metadata": {
        "id": "bPuSbrqJyze3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Deployment"
      ],
      "metadata": {
        "id": "oS3rzdJ41W5X"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wGdKQ_911WA_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}